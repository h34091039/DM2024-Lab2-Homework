{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 詹詠吏\n",
    "\n",
    "Student ID: 113065533\n",
    "\n",
    "GitHub ID: h34091039\n",
    "\n",
    "Kaggle name: yung-li, chan\n",
    "\n",
    "Kaggle private scoreboard snapshot: ![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here\n",
    "\n",
    "#import necessary package\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "import keras\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from TweetNormalizer import normalizeTweet\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AdamW\n",
    "\n",
    "import wordsegment\n",
    "from wordsegment import load, segment\n",
    "\n",
    "import re\n",
    "import inflect\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pki file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []\n",
    "with open('./dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as file:\n",
    "    for l in file:\n",
    "        datas.append(json.loads(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.read_csv(\"./dm-2024-isa-5810-lab-2-homework/emotion.csv\")\n",
    "data_mapping = pd.read_csv(\"./dm-2024-isa-5810-lab-2-homework/data_identification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                hashtags  tweet_id  \\\n",
       "0                             [Snapchat]  0x376b20   \n",
       "1          [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2                           [bibleverse]  0x28b412   \n",
       "3                                     []  0x1cd5b0   \n",
       "4                                     []  0x2de201   \n",
       "...                                  ...       ...   \n",
       "1867530  [mixedfeeling, butimTHATperson]  0x316b80   \n",
       "1867531                               []  0x29d0cb   \n",
       "1867532                               []  0x2a6a4f   \n",
       "1867533                               []  0x24faed   \n",
       "1867534                    [Sundayvibes]  0x34be8c   \n",
       "\n",
       "                                                      text  \n",
       "0        People who post \"add me on #Snapchat\" must be ...  \n",
       "1        @brianklaas As we see, Trump is dangerous to #...  \n",
       "2        Confident of your obedience, I write to you, k...  \n",
       "3                      Now ISSA is stalking Tasha 😂😂😂 <LH>  \n",
       "4        \"Trust is not the same as faith. A friend is s...  \n",
       "...                                                    ...  \n",
       "1867530  When you buy the last 2 tickets remaining for ...  \n",
       "1867531  I swear all this hard work gone pay off one da...  \n",
       "1867532  @Parcel2Go no card left when I wasn't in so I ...  \n",
       "1867533  Ah, corporate life, where you can date <LH> us...  \n",
       "1867534             Blessed to be living #Sundayvibes <LH>  \n",
       "\n",
       "[1867535 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = pd.DataFrame(datas)\n",
    "source = D[\"_source\"].tolist()\n",
    "source = pd.DataFrame(source)['tweet'].tolist()\n",
    "source = pd.DataFrame(source)\n",
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>989</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>827</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-05-12 12:51:52</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>368</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2017-10-02 17:54:04</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>498</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-10-10 11:04:32</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>840</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-09-02 14:25:06</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>360</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-11-16 01:40:07</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         _score          _index           _crawldate   _type  \\\n",
       "0           391  hashtag_tweets  2015-05-23 11:42:47  tweets   \n",
       "1           433  hashtag_tweets  2016-01-28 04:52:09  tweets   \n",
       "2           232  hashtag_tweets  2017-12-25 04:39:20  tweets   \n",
       "3           376  hashtag_tweets  2016-01-24 23:53:05  tweets   \n",
       "4           989  hashtag_tweets  2016-01-08 17:18:59  tweets   \n",
       "...         ...             ...                  ...     ...   \n",
       "1867530     827  hashtag_tweets  2015-05-12 12:51:52  tweets   \n",
       "1867531     368  hashtag_tweets  2017-10-02 17:54:04  tweets   \n",
       "1867532     498  hashtag_tweets  2016-10-10 11:04:32  tweets   \n",
       "1867533     840  hashtag_tweets  2016-09-02 14:25:06  tweets   \n",
       "1867534     360  hashtag_tweets  2016-11-16 01:40:07  tweets   \n",
       "\n",
       "                                hashtags  tweet_id  \\\n",
       "0                             [Snapchat]  0x376b20   \n",
       "1          [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2                           [bibleverse]  0x28b412   \n",
       "3                                     []  0x1cd5b0   \n",
       "4                                     []  0x2de201   \n",
       "...                                  ...       ...   \n",
       "1867530  [mixedfeeling, butimTHATperson]  0x316b80   \n",
       "1867531                               []  0x29d0cb   \n",
       "1867532                               []  0x2a6a4f   \n",
       "1867533                               []  0x24faed   \n",
       "1867534                    [Sundayvibes]  0x34be8c   \n",
       "\n",
       "                                                      text identification  \\\n",
       "0        People who post \"add me on #Snapchat\" must be ...          train   \n",
       "1        @brianklaas As we see, Trump is dangerous to #...          train   \n",
       "2        Confident of your obedience, I write to you, k...           test   \n",
       "3                      Now ISSA is stalking Tasha 😂😂😂 <LH>          train   \n",
       "4        \"Trust is not the same as faith. A friend is s...           test   \n",
       "...                                                    ...            ...   \n",
       "1867530  When you buy the last 2 tickets remaining for ...           test   \n",
       "1867531  I swear all this hard work gone pay off one da...           test   \n",
       "1867532  @Parcel2Go no card left when I wasn't in so I ...           test   \n",
       "1867533  Ah, corporate life, where you can date <LH> us...          train   \n",
       "1867534             Blessed to be living #Sundayvibes <LH>          train   \n",
       "\n",
       "              emotion  \n",
       "0        anticipation  \n",
       "1             sadness  \n",
       "2                 NaN  \n",
       "3                fear  \n",
       "4                 NaN  \n",
       "...               ...  \n",
       "1867530           NaN  \n",
       "1867531           NaN  \n",
       "1867532           NaN  \n",
       "1867533           joy  \n",
       "1867534           joy  \n",
       "\n",
       "[1867535 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge data & label\n",
    "D=D.drop(columns='_source')\n",
    "D['hashtags'] = source['hashtags']\n",
    "D['tweet_id'] = source['tweet_id']\n",
    "D['text'] = source['text']\n",
    "merged_df = pd.merge(D, data_mapping, left_on='tweet_id', right_on='tweet_id', how='left')\n",
    "merged_df = pd.merge(merged_df, label, left_on='tweet_id', right_on='tweet_id', how='left')\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training data & test data\n",
    "training_data = merged_df[merged_df['identification']=='train']\n",
    "testing_data = merged_df[merged_df['identification']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pickle\n",
    "training_data.to_pickle(\"train_df.pkl\") \n",
    "testing_data.to_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pki file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.drop(columns=['_type', '_index', 'identification'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the histogram of the data\n",
    "labels = train_df['emotion'].unique()\n",
    "post_total = len(train_df)\n",
    "df1 = train_df.groupby(['emotion']).count()['text']\n",
    "df1 = df1.apply(lambda x: round(x*100/post_total,3))\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "plt.bar(df1.index,df1.values)\n",
    "\n",
    "#arrange\n",
    "plt.ylabel('% of instances')\n",
    "plt.xlabel('Emotion')\n",
    "plt.title('Emotion distribution')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess (Manual Data Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train_df):\n",
    "    '''\n",
    "    manual preprocessing method\n",
    "    1. remove tag\n",
    "    2. remove <LH>\n",
    "    3. split hashtag\n",
    "    '''\n",
    "    text_list = []\n",
    "    for i in train_df['text']:\n",
    "        sen = []\n",
    "        for t in i.split():\n",
    "            if '@' in t:\n",
    "                sen.append(t[:t.index('@')])\n",
    "                continue\n",
    "            elif t == '<LH>':\n",
    "                continue\n",
    "            elif '#' in t:\n",
    "                sen.append(t[:t.index('#')])\n",
    "                tokens = segment(t[t.index('#'):])\n",
    "                for t2 in tokens:\n",
    "                    sen.append(t2)\n",
    "            else:\n",
    "                sen.append(t)\n",
    "        merged_string = \" \".join(sen)\n",
    "        text_list.append(merged_string)\n",
    "    train_df['text'] = text_list\n",
    "    return train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess(train_df)\n",
    "X_test = preprocess(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize Method 1 (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_vectorizer = CountVectorizer(tokenizer=nltk.word_tokenize) \n",
    "# 1. Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "BOW_vectorizer.fit(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X_train = BOW_vectorizer.transform(X_train['text'])\n",
    "y_train = X_train['emotion']\n",
    "\n",
    "\n",
    "X_test = BOW_vectorizer.transform(X_test['text'])\n",
    "\n",
    "## take a look at data dimension is a good habit  :)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize Method 2 (glove-twitter-25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = inflect.engine()\n",
    "\n",
    "\n",
    "def convert_number(match):\n",
    "    '''Function to convert a number to words'''\n",
    "    number = match.group(2)  # Extract the number part\n",
    "    return f\"{match.group(1)} {p.number_to_words(number)}\"\n",
    "\n",
    "def vectorize(X_train):\n",
    "    '''\n",
    "    map the token into the pretrain vector\n",
    "    if not in pretrain model, do another preprocessing\n",
    "    1. remove &gt\n",
    "    2. convert emoji to token\n",
    "    3. remove symbol\n",
    "    '''\n",
    "    a = X_train['text'].iloc\n",
    "    X_train_vec = []\n",
    "    for i in a[:]:\n",
    "        vec = np.zeros(25)\n",
    "        result = i.split()\n",
    "        clean_text = ''\n",
    "        vec_num = 0\n",
    "        for i,sen in enumerate(result):\n",
    "            if sen == '&gt;':\n",
    "                sen = '>'\n",
    "            hasEmoji = False\n",
    "            emojisen = ''\n",
    "            for x in sen:\n",
    "                if x in emoji.EMOJI_DATA: \n",
    "                    x = emoji.demojize(x)\n",
    "                    hasEmoji = True\n",
    "                    emojisen+=x\n",
    "                    emojisen+=' '\n",
    "                else:\n",
    "                    emojisen+=x\n",
    "            sen = emojisen if hasEmoji else sen\n",
    "            \n",
    "            l = re.sub(r'([a-zA-Z])(\\d+)',convert_number, sen)\n",
    "            l = re.sub(r'[^\\w\\s]', '', l)\n",
    "            clean_text+=l\n",
    "            clean_text+= \" \"\n",
    "            \n",
    "        for s in clean_text:\n",
    "            s = s.lower()\n",
    "            if s and s!=\" \":\n",
    "                if s.isdigit():\n",
    "                    s=p.number_to_words(s)\n",
    "                if s in model:\n",
    "                    vec += np.array(model.get_vector(s))\n",
    "                    vec_num+=1\n",
    "        X_train_vec.append(vec/vec_num)\n",
    "    return X_train_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorize(X_train)\n",
    "X_test = vectorize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = train_df['text'].tolist()\n",
    "\n",
    "X_train = tokenizer(\n",
    "\n",
    "    training_data,\n",
    "\n",
    "    max_length=8,\n",
    "\n",
    "    padding='max_length',\n",
    "\n",
    "    truncation=True,\n",
    "\n",
    "    return_tensors='pt'\n",
    "\n",
    ")\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "testing_data = test_df['text'].tolist()\n",
    "\n",
    "X_test = tokenizer(\n",
    "\n",
    "    testing_data,\n",
    "\n",
    "    max_length=8,\n",
    "\n",
    "    padding='max_length',\n",
    "\n",
    "    truncation=True,\n",
    "\n",
    "    return_tensors='pt'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess(BERTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = []\n",
    "for i in train_df['text'].tolist():\n",
    "    normalize.append(normalizeTweet(i))\n",
    "train_df['text'] = normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = train_df['text'].tolist()\n",
    "\n",
    "X_train = tokenizer(\n",
    "\n",
    "    training_data,\n",
    "\n",
    "    max_length=8,\n",
    "\n",
    "    padding='max_length',\n",
    "\n",
    "    truncation=True,\n",
    "\n",
    "    return_tensors='pt'\n",
    "\n",
    ")\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "testing_data = test_df['text'].tolist()\n",
    "\n",
    "X_test = tokenizer(\n",
    "\n",
    "    testing_data,\n",
    "\n",
    "    max_length=8,\n",
    "\n",
    "    padding='max_length',\n",
    "\n",
    "    truncation=True,\n",
    "\n",
    "    return_tensors='pt'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train_sample = label_encode(label_encoder, y_train)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train_sample[0:4])\n",
    "print('\\ny_train.shape: ', y_train_sample.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "pred = DT_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inputs, labels):\n",
    "\n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return {key: val[idx] for key, val in self.inputs.items()}, self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=8)\n",
    "\n",
    "dataset = CustomDataset(X_train, y_train_sample)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2048)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "\n",
    "import tqdm \n",
    "\n",
    "for epoch in range(3):\n",
    "\n",
    "    for batch in tqdm.tqdm(dataloader, desc = 'processing...'):\n",
    "\n",
    "        inputs, labels = batch\n",
    "\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "testset = CustomDataset(X_test, train_df['emotion'].tolist()[:411972])\n",
    "\n",
    "dataloader_test = DataLoader(testset, batch_size=2048)\n",
    "pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm.tqdm(dataloader_test, desc = 'processing...'):\n",
    "\n",
    "        inputs, labels = batch\n",
    "    \n",
    "        outputs = model(**inputs)\n",
    "        for out in  outputs.logits:\n",
    "            predictions = torch.argmax(out)\n",
    "            pred.append(predictions)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=8)\n",
    "\n",
    "dataset = CustomDataset(X_train, y_train_sample)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2048)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "\n",
    "import tqdm \n",
    "\n",
    "for epoch in range(3):\n",
    "\n",
    "    for batch in tqdm.tqdm(dataloader, desc = 'processing...'):\n",
    "\n",
    "        inputs, labels = batch\n",
    "\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "testset = CustomDataset(X_test, train_df['emotion'].tolist()[:411972])\n",
    "\n",
    "dataloader_test = DataLoader(testset, batch_size=2048)\n",
    "pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm.tqdm(dataloader_test, desc = 'processing...'):\n",
    "\n",
    "        inputs, labels = batch\n",
    "    \n",
    "        outputs = model(**inputs)\n",
    "        for out in  outputs.logits:\n",
    "            predictions = torch.argmax(out)\n",
    "            pred.append(predictions)\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output in csv Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "t = test_df['tweet_id'].tolist()\n",
    "emo = label_encoder.classes_\n",
    "\n",
    "ctr = 0\n",
    "for i,y in enumerate(pred):\n",
    "        output[ctr] = {'id': t[ctr], 'emotion': emo[y]}\n",
    "        ctr+=1\n",
    "output = pd.DataFrame.from_dict(output, orient='index')\n",
    "output.to_csv('output.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result (private)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### manual data cleaning + BOW + decision tree: 0.298\n",
    "#### manual data cleaning + glove-twitter-25 + decision tree: 0.354\n",
    "#### BERT(tokenizer+model): 0.373\n",
    "#### BERTweet(tokenizer+model): 0.467\n",
    "#### BERTweet(tweetnormalization+tokenizer+model): 0.445\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About the dataset\n",
    "    --tweet has some terms that is not standard use (difficult to get sementic meaning)\n",
    "    --tweet has hashtag and tags that may be not relevant for classification\n",
    "##### As the preprocess method got more complex, the results get better\n",
    "    --changing BOW to the pretrained weight for the model that use tweets as training set increase the accuracy\n",
    "##### BERTweet is a BERT model more focus on tweet, it's not surprising that it's performance is the best\n",
    "    --however, applying tweetnormalization drops the accuacy\n",
    "    --it may because the tweetnormalization segment the token, and the max length limitation cause the important information being truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other things tried"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sampling: The result is not good and doesn't save much time\n",
    "##### Dealing with unbalanced data by SMOTE: The result is not good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
